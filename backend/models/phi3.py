import logging
import torch
import nltk
from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel

# âœ… Configure logging
logging.basicConfig(level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s")

# âœ… Ensure NLTK punkt is available
nltk.download("punkt")


# âœ… Load Tokenizer and Model Globally
logging.info("ğŸ”„ Loading fine-tuned Phi-3 model and tokenizer...")

try:
    tokenizer = AutoTokenizer.from_pretrained("fine_tuned_phi3")
    base_model = AutoModelForCausalLM.from_pretrained(
        "microsoft/phi-3-mini-4k-instruct",
        torch_dtype=torch.float32,  # Use float32 for CPU compatibility
        device_map="cpu",  
        trust_remote_code=True,
        use_cache=False
    )
    
    model = PeftModel.from_pretrained(base_model, "fine_tuned_phi3").merge_and_unload()
    
    logging.info("âœ… Model and tokenizer loaded successfully!")

except Exception as e:
    logging.error(f"âŒ Error loading model/tokenizer: {e}")
    raise SystemExit("Failed to load model or tokenizer.")

# âœ… Function to generate response
def generate_response(context, user_query, max_new_tokens=50):
    """Generate a response using the fine-tuned Phi-3 model."""
    logging.info("ğŸ”„ Generating response using fine-tuned Phi-3...")
    
    try:
        # âœ… Log input details
        logging.info(f"ğŸ“¥ Received query: '{user_query}'")
        logging.info(f"ğŸ“š Context length: {len(context.split())} words")
        
        # âœ… Prepare input text
        input_text = f"Context: {context}\nQuery: {user_query}\nAnswer:"
        logging.info("âœï¸ Tokenizing input...")

        # âœ… Tokenize input
        inputs = tokenizer(input_text, return_tensors="pt", padding=True, truncation=True).to("cpu")
        input_ids = inputs["input_ids"][:, -64:]  # âœ… Reduce to last 64 tokens

        # âœ… Log tokenized input details
        logging.info(f"ğŸ”¢ Tokenized input size: {input_ids.shape}")

        # âœ… Generate response
        logging.info("ğŸ¤– Running model inference...")
        with torch.inference_mode():  
            output = model.generate(
                input_ids=input_ids,
                max_new_tokens=20,  
                pad_token_id=tokenizer.eos_token_id,
                num_beams=1,  # âœ… Faster generation
                do_sample=True,
                temperature=0.3,  
                top_p=0.7,  
                return_dict_in_generate=True,  
                output_scores=False,  # âŒ Disabled unnecessary computation
                use_cache=False  
            )

        # âœ… Decode response
        response_text = tokenizer.decode(output.sequences[0], skip_special_tokens=True)
        logging.info(f"ğŸ“œ Raw generated response: {response_text}")

        # âœ… Extract the final answer
        answer_start = response_text.find("Answer:")
        raw_answer = response_text[answer_start + len("Answer:"):].strip() if answer_start != -1 else response_text.strip()

        # âœ… Format response properly
        try:
            sentences = nltk.sent_tokenize(raw_answer)
            final_answer = " ".join(sentences[: min(len(sentences), 5)])
        except Exception as e:
            logging.error(f"âŒ Error in sentence tokenization: {str(e)}")
            final_answer = raw_answer  

        logging.info(f"âœ… Final extracted answer: {final_answer}")

        return final_answer

    except Exception as e:
        logging.error(f"âŒ Error generating response: {str(e)}")
        return "Error generating response."